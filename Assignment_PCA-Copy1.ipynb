{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc5603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn.metrics import silhouette_score as sil, calinski_harabasz_score as chs, silhouette_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ca57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supressing Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815c6595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data display customization\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7560849d",
   "metadata": {},
   "source": [
    "## 1. Case Summary\n",
    "### Perform Principal component analysis and perform clustering using first 3 principal component scores (both <font color='red'>Heirarchical and k mean clustering</font>(scree plot or elbow curve) and obtain optimum number of clusters and check whether we have obtained same number of clusters with the original data (class column we have ignored at the begining who shows it has 3 clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7e5205",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing dataset\n",
    "data = pd.read_csv('Wine.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88948186",
   "metadata": {},
   "source": [
    "## 2. Data Exploration <a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2850d75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eaa7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72421800",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9043681",
   "metadata": {},
   "source": [
    "##### Dropping Type Feature as it is a classifier that we have three types of wines which we need to find in clustering by keeping it will affect our inferences and further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6978bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ID'] = range(1, 179, 1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6994e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.drop('Type',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4b8c07",
   "metadata": {},
   "source": [
    "#### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40e08dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4268c1",
   "metadata": {},
   "source": [
    "##### Looking for some statistical information about each feature, we can see that the features have very diferrent scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdd03a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd16312",
   "metadata": {},
   "source": [
    "### 2.1 Missing Values <a class=\"anchor\" id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aa87b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# As a part of the Data cleansing we check the data for any missing/ na values\n",
    "# null count for columns\n",
    "\n",
    "null_count_col = df.isnull().sum().value_counts(ascending=False)\n",
    "\n",
    "# null percentage for columns\n",
    "\n",
    "null_percent_col = (df.isnull().sum() * 100 / len(df)).value_counts(ascending=False)\n",
    "\n",
    "print(\"Null Count for Columns:\\n\\n\", null_count_col, \"\\n\")\n",
    "print(\"Null Percentage for Columns:\\n\\n\", null_percent_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd714be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null count for rows\n",
    "\n",
    "null_count_row = df.isnull().sum(axis=1).value_counts(ascending=False)\n",
    "\n",
    "# null percentage for rows\n",
    "\n",
    "null_percent_row = (df.isnull().sum(axis=1) * 100 / len(df)).value_counts(ascending=False)\n",
    "\n",
    "print(\"Null Count for Rows:\\n\\n\", null_count_row, \"\\n\")\n",
    "print(\"Null Percentage for Rows:\\n\\n\", null_percent_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e44282",
   "metadata": {},
   "source": [
    "### Duplicated Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f88fae",
   "metadata": {},
   "source": [
    "#### print the duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1787e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additionally we check the data for any duplicate values, now this can be an optional check depending on the data being used\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29149ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b764124",
   "metadata": {},
   "source": [
    "#### There are no missing / Null and Duplicated  values  either in columns or rows, so we can move on to the next step, which is Exploratory Data Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c054e729",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis<a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4373c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4b66d",
   "metadata": {},
   "source": [
    "### Plotting the histogram of each numerical variable (in this case, all features), the main idea here is to visualize the data distribution for each feature. This method can bring fast insights as:\n",
    "+ Check the kind of each feature distribution\n",
    "+ Check data symmetry\n",
    "+ Verify features frequency\n",
    "+ Identify outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3659c35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='dark',font_scale=1.3, rc={'figure.figsize':(20,20)})\n",
    "ax=df.hist(bins=20,color='blue' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2688cb",
   "metadata": {},
   "source": [
    "### 3.1 Outliers Detection<a class=\"anchor\" id=\"3.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c271811",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87d1559",
   "metadata": {},
   "outputs": [],
   "source": [
    "ot=df.copy() \n",
    "fig, axes=plt.subplots(13,1,figsize=(20,20),sharex=False,sharey=False)\n",
    "sns.boxplot(x='Alcohol',data=ot,palette='crest',ax=axes[0])\n",
    "sns.boxplot(x='Malic',data=ot,palette='crest',ax=axes[1])\n",
    "sns.boxplot(x='Ash',data=ot,palette='crest',ax=axes[2])\n",
    "sns.boxplot(x='Alcalinity',data=ot,palette='crest',ax=axes[3])\n",
    "sns.boxplot(x='Magnesium',data=ot,palette='crest',ax=axes[4])\n",
    "sns.boxplot(x='Phenols',data=ot,palette='crest',ax=axes[5])\n",
    "sns.boxplot(x='Flavanoids',data=ot,palette='crest',ax=axes[6])\n",
    "sns.boxplot(x='Nonflavanoids',data=ot,palette='crest',ax=axes[7])\n",
    "sns.boxplot(x='Proanthocyanins',data=ot,palette='crest',ax=axes[8])\n",
    "sns.boxplot(x='Color',data=ot,palette='crest',ax=axes[9])\n",
    "sns.boxplot(x='Hue',data=ot,palette='crest',ax=axes[10])\n",
    "sns.boxplot(x='Dilution',data=ot,palette='crest',ax=axes[11])\n",
    "sns.boxplot(x='Proline',data=ot,palette='crest',ax=axes[12])\n",
    "plt.tight_layout(pad=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e382a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot( kind = 'box', subplots = True, layout = (4,4), sharex = False, sharey = False,color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5d27fa",
   "metadata": {},
   "source": [
    "## 4. Data Visualization<a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29955ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df,palette=\"dark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d23ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation heatmap\n",
    "\n",
    "f,ax = plt.subplots(figsize=(18,12))\n",
    "sns.heatmap(df.corr(), annot=True, linewidths =.5, fmt ='.1f',ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0edec26",
   "metadata": {},
   "source": [
    "<b>Unique points in this correlation matrix:</b>\n",
    "\n",
    "+ Phenols is positively correlated with Flavanoids, Dilution and Proanthocyanins\n",
    "+ Flavanoids is positively correlated with Proanthocyanins and Dilution\n",
    "+ Dilution is positively correlated with Hue\n",
    "+ Alcohol is positively correlated with Proline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b51550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting frequent flying Phenols vs. Flavanoids  \n",
    "sns.set(style='white', rc={'figure.figsize':(9,6)},font_scale=1.1)\n",
    "\n",
    "plt.scatter(x=df['Phenols'], y=df['Flavanoids'], color='blue',lw=0.1)\n",
    "plt.xlabel('Phenols')\n",
    "plt.ylabel('Flavanoids')\n",
    "plt.title('Data represented by the 2 strongest positively Correlated features',fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7798398b",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing<a class=\"anchor\" id=\"5\"></a>\n",
    "\n",
    "### 5.1) Applying Standard Scaler on the Data<a class=\"anchor\" id=\"5.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b799949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "std_df = standard_scaler.fit_transform(df)\n",
    "std_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70390c93",
   "metadata": {},
   "source": [
    "### 5.2) Applying MinMax Scaler on Dataset<a class=\"anchor\" id=\"5.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0643e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Minmaxscaler for accuracy result comparison\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "minmax = MinMaxScaler()\n",
    "\n",
    "minmax_df = minmax.fit_transform(df)\n",
    "minmax_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e656f6",
   "metadata": {},
   "source": [
    "## 6. PCA<a class=\"anchor\" id=\"6\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b053fe46",
   "metadata": {},
   "source": [
    "### 6.1 PCA on Standard Scaled Dataset<a class=\"anchor\" id=\"6.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78329905",
   "metadata": {},
   "source": [
    "##### Explained variance\n",
    "\n",
    "Explained variance shows how much of the variance/spread of the data is captured in each dimension, i.e. how important each additional principal component is to the original data representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca_var = PCA()\n",
    "pca_var.fit(std_df)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "xi = np.arange(1, 1+std_df.shape[1], step=1)\n",
    "yi = np.cumsum(pca_var.explained_variance_ratio_)\n",
    "plt.plot(xi, yi, marker='o', linestyle='--', color='b')\n",
    "\n",
    "# Aesthetics\n",
    "plt.ylim(0.0,1.1)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(1, 1+std_df.shape[1], step=1))\n",
    "plt.ylabel('Cumulative variance (%)')\n",
    "plt.title('Explained variance by each component')\n",
    "plt.axhline(y=1, color='r', linestyle='-')\n",
    "plt.gca().xaxis.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28708e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the amount of variance that each PCA explains is \n",
    "var = pca_var.explained_variance_ratio_\n",
    "var "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(1, len(var)+1),var)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('variance (%)')\n",
    "plt.title('Explained variance by each component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dfd457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative Variance\n",
    "var1 = np.cumsum(np.round(var,decimals = 4)*100)\n",
    "var1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419fe0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components=3)\n",
    "pca_std = pca.fit_transform(std_df)\n",
    "\n",
    "# Convert to data frame\n",
    "pca_std_df = pd.DataFrame(data = pca_std, columns = ['PC1', 'PC2','PC3'])\n",
    "\n",
    "# Shape and preview\n",
    "print(pca_std_df.shape)\n",
    "pca_std_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4862f018",
   "metadata": {},
   "source": [
    "#### PCA plot in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4524e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure size\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Scatterplot\n",
    "plt.scatter(pca_std_df.iloc[:,0], pca_std_df.iloc[:,1], s=40)\n",
    "\n",
    "# Aesthetics\n",
    "plt.title('PCA plot in 2D using Strongest Principle Components')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a7bfa4",
   "metadata": {},
   "source": [
    "### 6.2 PCA on MinMax Scaled Dataset<a class=\"anchor\" id=\"6.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa8f669",
   "metadata": {},
   "source": [
    "##### Explained variance\n",
    "\n",
    "Explained variance shows how much of the variance/spread of the data is captured in each dimension, i.e. how important each additional principal component is to the original data representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6f5e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca_var = PCA()\n",
    "pca_var.fit(minmax_df)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "xi = np.arange(1, 1+minmax_df.shape[1], step=1)\n",
    "yi = np.cumsum(pca_var.explained_variance_ratio_)\n",
    "plt.plot(xi, yi, marker='o', linestyle='--', color='b')\n",
    "\n",
    "# Aesthetics\n",
    "plt.ylim(0.0,1.1)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(1, 1+minmax_df.shape[1], step=1))\n",
    "plt.ylabel('Cumulative variance (%)')\n",
    "plt.title('Explained variance by each component')\n",
    "plt.axhline(y=1, color='r', linestyle='-')\n",
    "plt.gca().xaxis.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cecf3b",
   "metadata": {},
   "source": [
    "It is interesting to see that with just 2 components we capture over 50% of the variance. And by moving up to 3 components, we get 2 thirds of the variance.\n",
    "\n",
    "For this reason, sometimes PCA is used for feature selection. For very big datasets with hundreds of features, it can take a long time to train models with some features not adding much value. A solution is to reduce the number of columns by using principal components from PCA. The number of components is normally chosen by looking at the explained variance graph (e.g. a minimum of 60% might be required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b8e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the amount of variance that each PCA explains is \n",
    "var = pca_var.explained_variance_ratio_\n",
    "var "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(1, len(var)+1),var)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('variance (%)')\n",
    "plt.title('Explained variance by each component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330dabdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative Variance\n",
    "var1 = np.cumsum(np.round(var,decimals = 4)*100)\n",
    "var1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab4c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components=3)\n",
    "pca_minmax = pca.fit_transform(minmax_df)\n",
    "\n",
    "# Convert to data frame\n",
    "pca_minmax_df = pd.DataFrame(data = pca_minmax, columns = ['PC1', 'PC2','PC3'])\n",
    "\n",
    "# Shape and preview\n",
    "print(pca_minmax_df.shape)\n",
    "pca_minmax_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e653cd8",
   "metadata": {},
   "source": [
    "##### The new columns are called Principal Components (PC's) and give the coordinates of the data in the new smaller space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5915238",
   "metadata": {},
   "source": [
    "#### PCA plot in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1195049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure size\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Scatterplot\n",
    "plt.scatter(pca_minmax_df.iloc[:,0], pca_minmax_df.iloc[:,1], s=40)\n",
    "\n",
    "# Aesthetics\n",
    "plt.title('PCA plot in 2D using Strongest Principle Components')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc01808",
   "metadata": {},
   "source": [
    "##### Notes:\n",
    "\n",
    "+ Notice how these points have <b>no colours</b> (yet) because we don't know what their classes are. This is what makes it an <b>unsupervised method.</b>\n",
    "+ We know our dataset falls naturally into 3 classes (3 types of wine), so we assign colours by <b>clustering</b> the data into 3 groups using techniques such as <b>k-Means</b> and <b>Hierarchical</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724437bb",
   "metadata": {},
   "source": [
    "## 7. KMeans Clustering<a class=\"anchor\" id=\"7\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596b453d",
   "metadata": {},
   "source": [
    "### 7.1 Elbow Method for Determining Cluster Amount on PCA Standard Scaled Dataset<a class=\"anchor\" id=\"7.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251141a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "for i in tqdm(range(2,10)):\n",
    "    kmeans = KMeans(n_clusters=i,\n",
    "               init='k-means++',\n",
    "               n_init=15,\n",
    "               max_iter=500,\n",
    "               random_state=17)\n",
    "    kmeans.fit(pca_std_df)\n",
    "    inertia.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce1605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette = {}\n",
    "for i in tqdm(range(2,10)):\n",
    "    kmeans = KMeans(n_clusters=i,\n",
    "               init='k-means++',\n",
    "               n_init=15,\n",
    "               max_iter=500,\n",
    "               random_state=17)\n",
    "    kmeans.fit(pca_std_df)\n",
    "    silhouette[i] = sil(pca_std_df, kmeans.labels_, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b164cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\", rc={'figure.figsize':(13,8)}, font_scale=2)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(2,len(inertia)+2), inertia, marker=\"*\", lw=2, color=\"skyblue\", ms=10)\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.title(\"K-Means Inertia\", fontweight='bold')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.bar(range(len(silhouette)), list(silhouette.values()), align='center', width=.7, edgecolor=\"black\", lw=2, color=\"skyblue\" )\n",
    "plt.xticks(range(len(silhouette)), list(silhouette.keys()))\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.title(\"Silhouette Score\", fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a58633",
   "metadata": {},
   "source": [
    "##### There is a clearly visible \"elbow\" . A choice of 3 clusters seems to be fair the maximum silhouette score also indicates that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b3aaa3",
   "metadata": {},
   "source": [
    "### 7.2 Silhouette Score<a class=\"anchor\" id=\"7.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1428f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "n_clusters = [2,3,4,5,6,7,8,9,10] # number of clusters\n",
    "clusters_inertia = [] # inertia of clusters\n",
    "s_scores = [] # silhouette scores\n",
    "\n",
    "for n in n_clusters:\n",
    "    KM_est = KMeans(n_clusters=n, init='k-means++').fit(pca_std_df)\n",
    "    clusters_inertia.append(KM_est.inertia_)    # data for the elbow method\n",
    "    silhouette_avg = silhouette_score(pca_std_df, KM_est.labels_)\n",
    "    s_scores.append(silhouette_avg) # data for the silhouette score method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190d70a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax = sns.lineplot(n_clusters, s_scores, marker='o', ax=ax)\n",
    "ax.set_title(\"Silhouette score method\")\n",
    "ax.set_xlabel(\"number of clusters\")\n",
    "ax.set_ylabel(\"Silhouette score\")\n",
    "ax.axvline(3, ls=\"--\", c=\"red\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb3320",
   "metadata": {},
   "source": [
    "##### Silhouette score method indicates the best options would be respectively 3 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2931af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. How many number of clusters? n_clusters?\n",
    "\n",
    "# Since true labels are not known..we will use Silhouette Coefficient (Clustering performance evaluation)\n",
    "# knee Elbow graph method\n",
    "\n",
    "\n",
    "# Instantiate a scikit-learn K-Means model. we will check for two diff hyperparameters value effect.\n",
    "model = KMeans(random_state=10, max_iter=500, init='k-means++')\n",
    "\n",
    "# Instantiate the KElbowVisualizer with the number of clusters and the metric\n",
    "visualizer = KElbowVisualizer(model, k=(2,20), metric='silhouette', timings=False)\n",
    "# Fit the data and visualize\n",
    "print('Elbow Plot for Standard Scaler data')\n",
    "visualizer.fit(pca_std_df)    \n",
    "visualizer.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f15505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the elbow method, the ideal number of clusters to use was 6.\n",
    "# We will also use the Silhouette score to determine an optimal number.\n",
    "\n",
    "clust_list = [2,3,4,5,6,7,8,9]\n",
    "\n",
    "#  Silhouette score for stadardScaler applied on data.\n",
    "\n",
    "for n_clusters in clust_list:\n",
    "    clusterer1 = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    cluster_labels1 = clusterer1.fit_predict(pca_std_df)\n",
    "    sil_score1= sil(pca_std_df, cluster_labels1)\n",
    "    print(\"For n_clusters =\", n_clusters,\"The average silhouette_score is :\", sil_score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51825936",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "range_n_clusters = [2,3,4,5,6,7,8,9]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(pca_std_df) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    cluster_labels = clusterer.fit_predict(pca_std_df)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = sil(pca_std_df, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(pca_std_df, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(pca_std_df.iloc[:,0], pca_std_df.iloc[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:,0], centers[:,1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"PCA on Standard Scaled Dataset\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6894b3d",
   "metadata": {},
   "source": [
    "####  Conclusion:\n",
    "According the the silhouette score of:\n",
    "\n",
    "The standardized data, the ideal <b>number of clusters is 3</b>, with a score higher than other options, of <b>0.48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0f626c",
   "metadata": {},
   "source": [
    "## 7.3 Build KMeans Cluster algorithm using K=3 and PCA on Standard Scaler Applied Dataset <a class=\"anchor\" id=\"7.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be47aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have found good number of cluster = 3\n",
    "# model building using cluster numbers = 3\n",
    "\n",
    "model_kmeans = KMeans(n_clusters=3, random_state=0, init='k-means++')\n",
    "y_predict_kmeans = model_kmeans.fit_predict(pca_std_df)\n",
    "y_predict_kmeans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39ecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are nothing but cluster labels...\n",
    "\n",
    "y_predict_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8ae5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c817976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster centres associated with each lables\n",
    "\n",
    "model_kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c861b4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# within-cluster sum of squared\n",
    "\n",
    "# The lower values of inertia are better and zero is optimal.\n",
    "# Inertia is the sum of squared error for each cluster. \n",
    "# Therefore the smaller the inertia the denser the cluster(closer together all the points are)\n",
    "\n",
    "model_kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275edf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign clusters to the data set\n",
    "data['Kmeans_label'] = model_kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365707b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by Clusters (K=6)\n",
    "data.groupby('Kmeans_label').agg(['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae09947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(1,2,sharey=False)\n",
    "fig.set_size_inches(15,6)\n",
    "\n",
    "\n",
    "\n",
    "sil_visualizer1 = SilhouetteVisualizer(model_kmeans,ax= ax1, colors=['#922B21','#5B2C6F','#1B4F72'])\n",
    "sil_visualizer1.fit(principal_df)\n",
    "\n",
    "\n",
    "# 2nd Plot showing the actual clusters formed\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "colors1 = cm.nipy_spectral(model_kmeans.labels_.astype(float) / 3) # 3 is number of clusters\n",
    "ax2.scatter(pca_std_df.iloc[:, 0], pca_std_df.iloc[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors1, edgecolor='k')\n",
    "\n",
    "# Labeling the clusters\n",
    "centers1 = model_kmeans.cluster_centers_\n",
    "# Draw white circles at cluster centers\n",
    "ax2.scatter(centers1[:, 0], centers1[:, 1], marker='o',c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "for i, c in enumerate(centers1):\n",
    "    ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,s=50, edgecolor='k')\n",
    "\n",
    "\n",
    "ax2.set_title(label =\"The visualization of the clustered data.\")\n",
    "ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % 3),fontsize=14, fontweight='bold')\n",
    "\n",
    "#sil_visualizer1.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5914963b",
   "metadata": {},
   "source": [
    "### 7.4 Elbow Method and Silhouette Score on PCA MinMaxScaler Applied Data<a class=\"anchor\" id=\"7.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f02499",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "for i in tqdm(range(2,10)):\n",
    "    kmeans = KMeans(n_clusters=i,\n",
    "               init='k-means++',\n",
    "               n_init=15,\n",
    "               max_iter=500,\n",
    "               random_state=17)\n",
    "    kmeans.fit(pca_minmax_df)\n",
    "    inertia.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e039e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette = {}\n",
    "for i in tqdm(range(2,10)):\n",
    "    kmeans = KMeans(n_clusters=i,\n",
    "               init='k-means++',\n",
    "               n_init=15,\n",
    "               max_iter=500,\n",
    "               random_state=17)\n",
    "    kmeans.fit(pca_minmax_df)\n",
    "    silhouette[i] = sil(pca_minmax_df, kmeans.labels_, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8115ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\", rc={'figure.figsize':(13,8)}, font_scale=2)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(2,len(inertia)+2), inertia, marker=\"*\", lw=2, color=\"skyblue\", ms=10)\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.title(\"K-Means Inertia\", fontweight='bold')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.bar(range(len(silhouette)), list(silhouette.values()), align='center', width=.7, edgecolor=\"black\", lw=2, color=\"skyblue\" )\n",
    "plt.xticks(range(len(silhouette)), list(silhouette.keys()))\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.title(\"Silhouette Score\", fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dec01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss=[]\n",
    "for i in range (1,9):\n",
    "    kmeans=KMeans(n_clusters=i,random_state=2)\n",
    "    kmeans.fit(pca_minmax_df)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "# Plot K values range vs WCSS to get Elbow graph for choosing K (no. of clusters)\n",
    "plt.plot(range(1,9),wcss,color = 'black')\n",
    "plt.scatter(range(1,9),wcss,color='red')\n",
    "plt.title('Elbow Graph for MinMaxScaler')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da391c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "n_clusters = [2,3,4,5,6,7,8,9,10] # number of clusters\n",
    "clusters_inertia = [] # inertia of clusters\n",
    "s_scores = [] # silhouette scores\n",
    "\n",
    "for n in n_clusters:\n",
    "    KM_est = KMeans(n_clusters=n, init='k-means++').fit(pca_minmax_df)\n",
    "    clusters_inertia.append(KM_est.inertia_)    # data for the elbow method\n",
    "    silhouette_avg = silhouette_score(pca_minmax_df, KM_est.labels_)\n",
    "    s_scores.append(silhouette_avg) # data for the silhouette score method\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax = sns.lineplot(n_clusters, s_scores, marker='o', ax=ax)\n",
    "ax.set_title(\"Silhouette score method\")\n",
    "ax.set_xlabel(\"number of clusters\")\n",
    "ax.set_ylabel(\"Silhouette score\")\n",
    "ax.axvline(3, ls=\"--\", c=\"red\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94929c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a scikit-learn K-Means model. we will check for two diff hyperparameters value effect.\n",
    "model = KMeans(random_state=10, max_iter=500, init='k-means++')\n",
    "\n",
    "# Instantiate the KElbowVisualizer with the number of clusters and the metric\n",
    "visualizer = KElbowVisualizer(model, k=(2,20), metric='silhouette', timings=False)\n",
    "# Fit the data and visualize\n",
    "print('Elbow Plot for MinMaxScaler data')\n",
    "visualizer.fit(pca_minmax_df)    \n",
    "visualizer.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d814b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the elbow method, the ideal number of clusters to use was 6.\n",
    "# We will also use the Silhouette score to determine an optimal number.\n",
    "\n",
    "clust_list = [2,3,4,5,6,7,8,9]\n",
    "\n",
    "#  Silhouette score for MinMaxScaler Applied on data .\n",
    "\n",
    "for n_clusters in clust_list:\n",
    "    clusterer1 = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    cluster_labels1 = clusterer1.fit_predict(pca_minmax_df)\n",
    "    sil_score1= sil(pca_minmax_df, cluster_labels1)\n",
    "    print(\"For n_clusters =\", n_clusters,\"The average silhouette_score is :\", sil_score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5309765a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "range_n_clusters = [2,3,4,5,6,7,8,9]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(pca_minmax_df) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(pca_minmax_df)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = sil(pca_minmax_df, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(pca_minmax_df, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(pca_minmax_df.iloc[:,0], pca_minmax_df.iloc[:,1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:,0], centers[:,1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"PCA on  MinMax Scaler Dataset\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d013502b",
   "metadata": {},
   "source": [
    "## 7.5 Build KMeans Cluster algorithm using K=3 and PCA on MinMaxScaler Applied Dataset<a class=\"anchor\" id=\"7.5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have found good number of cluster = 3\n",
    "# model building using cluster numbers = 3\n",
    "\n",
    "model_kmeans = KMeans(n_clusters=3, random_state=0, init='k-means++')\n",
    "y_predict_kmeans = model_kmeans.fit_predict(pca_minmax_df)\n",
    "y_predict_kmeans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd0b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are nothing but cluster labels...\n",
    "\n",
    "y_predict_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0fa133",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14e8858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster centres associated with each lables\n",
    "\n",
    "model_kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572130e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# within-cluster sum of squared\n",
    "\n",
    "# The lower values of inertia are better and zero is optimal.\n",
    "# Inertia is the sum of squared error for each cluster. \n",
    "# Therefore the smaller the inertia the denser the cluster(closer together all the points are)\n",
    "\n",
    "model_kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1760d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign clusters to the data set\n",
    "\n",
    "df['Kmeans_label'] = model_kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0a4ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by Clusters (K=3)\n",
    "df.groupby('Kmeans_label').agg(['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada4a25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(1,2,sharey=False)\n",
    "fig.set_size_inches(15,6)\n",
    "\n",
    "\n",
    "\n",
    "sil_visualizer1 = SilhouetteVisualizer(model_kmeans,ax= ax1, colors=['#922B21','#5B2C6F','#1B4F72'])\n",
    "sil_visualizer1.fit(principal_df)\n",
    "\n",
    "\n",
    "# 2nd Plot showing the actual clusters formed\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "colors1 = cm.nipy_spectral(model_kmeans.labels_.astype(float) / 2) # 3 is number of clusters\n",
    "ax2.scatter(pca_minmax_df.iloc[:, 0], pca_minmax_df.iloc[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors1, edgecolor='k')\n",
    "\n",
    "# Labeling the clusters\n",
    "centers1 = model_kmeans.cluster_centers_\n",
    "# Draw white circles at cluster centers\n",
    "ax2.scatter(centers1[:, 0], centers1[:, 1], marker='o',c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "for i, c in enumerate(centers1):\n",
    "    ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,s=50, edgecolor='k')\n",
    "\n",
    "\n",
    "ax2.set_title(label =\"The visualization of the clustered data.\")\n",
    "ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % 4),fontsize=14, fontweight='bold')\n",
    "\n",
    "sil_visualizer1.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee38529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure size\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Scatterplot\n",
    "plt.scatter(pca_minmax_df.iloc[:,0], pca_minmax_df.iloc[:,1], c=data['Kmeans_label'], cmap=\"brg\", s=40)\n",
    "\n",
    "# Aesthetics\n",
    "plt.title('PCA plot in 2D')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6871d12",
   "metadata": {},
   "source": [
    "#### We can now clearly see the 3 number of clusters formed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa72cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = data.copy()\n",
    "df1['Cluster'] = model_kmeans.labels_\n",
    "\n",
    "aux=df1.columns.tolist()\n",
    "aux[0:len(aux)-1]\n",
    "\n",
    "sns.set(style=\"darkgrid\", rc={'figure.figsize':(13,8)}, font_scale=2)\n",
    "\n",
    "for cluster in aux[0:len(aux)-1]:\n",
    "    grid = sns.FacetGrid(df1, col=\"Cluster\")\n",
    "    grid.map(plt.hist, cluster, color=\"skyblue\", lw=1, edgecolor=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12468a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting elements based on cluster label assigned and taking average for insights.\n",
    "\n",
    "cluster1 = pd.DataFrame(df1.loc[df1.Cluster==0].mean(),columns= ['Cluster1_avg'])\n",
    "cluster2 = pd.DataFrame(df1.loc[df1.Cluster==1].mean(),columns= ['Cluster2_avg'])\n",
    "cluster3 = pd.DataFrame(df1.loc[df1.Cluster==2].mean(),columns= ['Cluster2_avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a988a1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df = pd.concat([cluster1,cluster2,cluster3],axis=1)\n",
    "avg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f8057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and plot one Column data .xs method\n",
    "for i , row in avg_df.iterrows():\n",
    "    fig = plt.subplots(figsize=(8,6))\n",
    "    j = avg_df.xs(i ,axis = 0)\n",
    "    plt.title(i, fontsize=16, fontweight=20)\n",
    "    j.plot(kind='bar',fontsize=14)\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5686f9f",
   "metadata": {},
   "source": [
    "### 7.6 Comparing Kmeans Clusters with the Original Classified Dataset using \"Type\" Feature<a class=\"anchor\" id=\"7.6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f70b283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot\n",
    "plt.scatter(pca_minmax_df.iloc[:,0], pca_minmax_df.iloc[:,1], c=df['Kmeans_label'], cmap=\"brg\", s=40)\n",
    "\n",
    "# Aesthetics\n",
    "plt.title('After Kmeans Clustering on PCA MinMax Scaled Dataset')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()\n",
    "\n",
    "# Scatterplot\n",
    "plt.scatter(pca_minmax_df.iloc[:,0], pca_minmax_df.iloc[:,1], c=data['Type'], cmap=\"brg\", s=40)\n",
    "\n",
    "# Aesthetics\n",
    "plt.title('Original Classification without Kmeans CLustering')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc86f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by Clusters (K=3)\n",
    "display('After Applying PCA and Kmens CLustering on Dataset',df.groupby('Kmeans_label').agg(['mean']),'Original Classified Dataset',data.groupby('Type').agg(['mean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cfb3e2",
   "metadata": {},
   "source": [
    "## 7.7 t-SNE<a class=\"anchor\" id=\"7.7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b93ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(minmax_df)\n",
    "\n",
    "# Convert to data frame\n",
    "tsne_df = pd.DataFrame(data = X_tsne, columns = ['tsne comp. 1', 'tsne comp. 2'])\n",
    "\n",
    "# Shape and preview\n",
    "print(tsne_df.shape)\n",
    "tsne_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac07307",
   "metadata": {},
   "source": [
    "##### t-SNE plot in 2D coloured by class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccd375c",
   "metadata": {},
   "source": [
    "Like PCA, t-SNE is an <b>unsupervised algorithm</b>, however we will use the same k-Means clusters from before to colour code the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c65df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure size\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Scatterplot\n",
    "plt.scatter(tsne_df.iloc[:,0], tsne_df.iloc[:,1], c=df['Kmeans_label'], cmap=\"brg\", s=40)\n",
    "\n",
    "# Aesthetics\n",
    "plt.title('t-SNE plot in 2D')\n",
    "plt.xlabel('tsne component 1')\n",
    "plt.ylabel('tsne component 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f72ca64",
   "metadata": {},
   "source": [
    "## 8 Hierarchical Clustering Algorithm<a class=\"anchor\" id=\"8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135ef2ce",
   "metadata": {},
   "source": [
    "### 8.1 Dendogram on PCA MinMaxScaler Applied Dataset<a class=\"anchor\" id=\"8.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66901ddf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Applying Dendrogram on data. Or you may apply it on Standardized/normalized indepedent variable data.\n",
    "# Here diffrent linkage method from hyperparameter is used to see diff between methods for understanding. \n",
    "# Ward method is commanly used since it is simpler to visualize understanding.\n",
    "# Find number of cluster's using color coding of dendrogram. Each color indicates one cluster.\n",
    "\n",
    "for methods in ['single','complete','average','weighted','centroid','median','ward']: \n",
    "    plt.figure(figsize =(20, 6)) \n",
    "    \n",
    "    dict = {'fontsize':24,'fontweight' :16, 'color' : 'blue'}\n",
    "    \n",
    "    plt.title('Visualising the data, Method- {}'.format(methods),fontdict = dict) \n",
    "    Dendrogram1 = sch.dendrogram(sch.linkage(pca_minmax_df, method = methods,optimal_ordering=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79404887",
   "metadata": {},
   "source": [
    "### 8.2 Silhouette Score method for PCA MinMax Scaled Data <a class=\"anchor\" id=\"8.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a36af4",
   "metadata": {},
   "source": [
    "#### 8.2(A) Applying Different Linkages using Euclidean Method for distance Calculation<a class=\"anchor\" id=\"8.2A\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944682da",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = [2,3,4,5,6,7,8]  # always start number from 2.\n",
    "\n",
    "for n_clusters in n_clusters:\n",
    "    for linkages in [\"ward\", \"complete\", \"average\", \"single\"]:\n",
    "        hie_cluster1 = AgglomerativeClustering(n_clusters=n_clusters,linkage=linkages) # by default it takes linkage 'ward'\n",
    "        hie_labels1 = hie_cluster1.fit_predict(pca_minmax_df)\n",
    "        silhouette_score1 = sil(pca_minmax_df, hie_labels1)\n",
    "        print(\"For n_clusters =\", n_clusters,\"The average silhouette_score with linkage-\",linkages, ':',silhouette_score1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71cb9b",
   "metadata": {},
   "source": [
    "#### 8.2(B) Applying Different Linkages using Different Distance Methods<a class=\"anchor\" id=\"8.2A\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce72c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_clusters = [2,3,4,5,6,7,8]  # always start number from 2.\n",
    "\n",
    "for n_clusters in n_clusters:\n",
    "    for linkages in [\"complete\", \"average\", \"single\"]:\n",
    "        for affinities in [\"euclidean\", \"l1\", \"l2\",\"manhattan\", \"cosine\"]:\n",
    "            hie_cluster1 = AgglomerativeClustering(n_clusters=n_clusters,affinity=affinities,linkage=linkages) # by default it takes linkage 'ward'\n",
    "            hie_labels1 = hie_cluster1.fit_predict(pca_minmax_df)\n",
    "            silhouette_score1 = sil(pca_minmax_df, hie_labels1)\n",
    "            print(\"For n_clusters =\", n_clusters,\"The average silhouette_score with linkage-\",linkages,\"and Affinity-\",affinities,':',silhouette_score1)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc49844",
   "metadata": {},
   "source": [
    "### 8.3 Dendrogram on Standard Scaler Applied on Data<a class=\"anchor\" id=\"8.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb99748",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Applying Dendrogram on data. Or you may apply it on Standardized/normalized indepedent variable data.\n",
    "# Here diffrent linkage method from hyperparameter is used to see diff between methods for understanding. \n",
    "# Ward method is commanly used since it is simpler to visualize understanding.\n",
    "# Find number of cluster's using color coding of dendrogram. Each color indicates one cluster.\n",
    "\n",
    "for methods in ['single','complete','average','weighted','centroid','median','ward']: \n",
    "    plt.figure(figsize =(20, 6)) \n",
    "    \n",
    "    dict = {'fontsize':24,'fontweight' :16, 'color' : 'blue'}\n",
    "    \n",
    "    plt.title('Visualising the data, Method- {}'.format(methods),fontdict = dict) \n",
    "    Dendrogram1 = sch.dendrogram(sch.linkage(pca_std_df, method = methods,optimal_ordering=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eebc28",
   "metadata": {},
   "source": [
    "### 8.4 Silhouette Score method for PCA Standard Scaled Data <a class=\"anchor\" id=\"8.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34d8f42",
   "metadata": {},
   "source": [
    "#### 8.4(A) Applying Different Linkages using Euclidean Method for distance Calculation<a class=\"anchor\" id=\"8.4A\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61319fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = [2,3,4,5,6,7,8]  # always start number from 2.\n",
    "\n",
    "for n_clusters in n_clusters:\n",
    "    for linkages in [\"ward\", \"complete\", \"average\", \"single\"]:\n",
    "        hie_cluster1 = AgglomerativeClustering(n_clusters=n_clusters,linkage=linkages) # by default it takes linkage 'ward'\n",
    "        hie_labels1 = hie_cluster1.fit_predict(pca_std_df)\n",
    "        silhouette_score1 = sil(pca_std_df, hie_labels1)\n",
    "        print(\"For n_clusters =\", n_clusters,\"The average silhouette_score with linkage-\",linkages, ':',silhouette_score1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3162f32",
   "metadata": {},
   "source": [
    "#### 8.4(B) Applying Different Linkages using Different Distance Methods<a class=\"anchor\" id=\"8.4B\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2328f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = [2,3,4,5,6,7,8]  # always start number from 2.\n",
    "\n",
    "for n_clusters in n_clusters:\n",
    "    for linkages in [\"complete\", \"average\", \"single\"]:\n",
    "        for affinities in [\"euclidean\", \"l1\", \"l2\",\"manhattan\", \"cosine\"]:\n",
    "            hie_cluster1 = AgglomerativeClustering(n_clusters=n_clusters,affinity=affinities,linkage=linkages) # by default it takes linkage 'ward'\n",
    "            hie_labels1 = hie_cluster1.fit_predict(pca_std_df)\n",
    "            silhouette_score1 = sil(pca_std_df, hie_labels1)\n",
    "            print(\"For n_clusters =\", n_clusters,\"The average silhouette_score with linkage-\",linkages,\"and Affinity-\",affinities,':',silhouette_score1)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb6e381",
   "metadata": {},
   "source": [
    "### 8.5 Run Hierarchical Clustering.(Agglomerative Clustering) For PCA on Standard Scaled Data <a class=\"anchor\" id=\"8.5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f538ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_clustering = AgglomerativeClustering(n_clusters=4, linkage='complete', affinity='l1')\n",
    "y_pred_hie = agg_clustering.fit_predict(pca_std_df)\n",
    "print(y_pred_hie.shape)\n",
    "y_pred_hie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6fe84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster numbers\n",
    "\n",
    "agg_clustering.n_clusters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ae6ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering Score\n",
    "\n",
    "(sil(pca_std_df, agg_clustering.labels_)*100).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295b7b71",
   "metadata": {},
   "source": [
    "### Putting Cluster lables into original dataset And analysis of the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550ad346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concating Labels with main dataset copy\n",
    "\n",
    "df['Hierarchical_labels'] = agg_clustering.labels_\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6206995",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Hierarchical_labels').agg(['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54468a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting barplot using groupby method to get visualize how many row no. in each cluster\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df.groupby(['Hierarchical_labels']).count()['ID'].plot(kind='bar')\n",
    "plt.ylabel('ID Counts')\n",
    "plt.title('Hierarchical Clustering PCA Standard Scaled Data',fontsize='large',fontweight='bold')\n",
    "ax.set_xlabel('Clusters', fontsize='large', fontweight='bold')\n",
    "ax.set_ylabel('ID counts', fontsize='large', fontweight='bold')\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe265990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure size\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Scatterplot\n",
    "plt.scatter(pca_std_df.iloc[:,0], pca_std_df.iloc[:,1], c=df['Hierarchical_labels'], cmap=\"brg\", s=40)\n",
    "\n",
    "# Aesthetics\n",
    "plt.title('PCA plot in 2D')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d5e077",
   "metadata": {},
   "source": [
    "#### We cannot now clearly see the 4 number of clusters formed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf8bc4d",
   "metadata": {},
   "source": [
    "### 8.5(A) Comparing Hierarchical Clusters with the Original Classified Dataset using \"Type\" Feature<a class=\"anchor\" id=\"8.5A\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea26c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot\n",
    "plt.scatter(pca_std_df.iloc[:,0], pca_std_df.iloc[:,1], c=df['Hierarchical_labels'], cmap=\"brg\", s=40)\n",
    "\n",
    "# Aesthetics\n",
    "plt.title('After PCA Standard Scaler and Hierarchical Clustering plot in 2D')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()\n",
    "\n",
    "# Scatterplot\n",
    "plt.scatter(pca_std_df.iloc[:,0], pca_std_df.iloc[:,1], c=data['Type'], cmap=\"brg\", s=40)\n",
    "\n",
    "# Aesthetics\n",
    "plt.title('Original Classification without Hierarchical CLustering')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605aefda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by Clusters (Clusters=4)\n",
    "display('After Applying Hierarchical Clustering on PCA Standard Scaled Dataset',df.groupby('Hierarchical_labels').agg(['mean']),'Original Classified Dataset',data.groupby('Type').agg(['mean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac00e90",
   "metadata": {},
   "source": [
    "### 8.6 Run Hierarchical Clustering.(Agglomerative Clustering) For PCA on MinMaxScaled Data <a class=\"anchor\" id=\"8.6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b757f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_clustering = AgglomerativeClustering(n_clusters=3, linkage='average', affinity='l2')\n",
    "y_pred_hie = agg_clustering.fit_predict(pca_minmax_df)\n",
    "print(y_pred_hie.shape)\n",
    "y_pred_hie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6813b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster numbers\n",
    "\n",
    "agg_clustering.n_clusters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc22f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering Score\n",
    "\n",
    "(sil(pca_minmax_df, agg_clustering.labels_)*100).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da2c83b",
   "metadata": {},
   "source": [
    "### Putting Cluster lables into original dataset And analysis of the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba538a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concating Labels with main dataset copy\n",
    "\n",
    "df['Hierarchical_labels'] = agg_clustering.labels_\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd9ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Hierarchical_labels').agg(['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37221461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting barplot using groupby method to get visualize how many row no. in each cluster\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df.groupby(['Hierarchical_labels']).count()['ID'].plot(kind='bar')\n",
    "plt.ylabel('ID Counts')\n",
    "plt.title('Hierarchical Clustering PCA MinMax Scaled Data',fontsize='large',fontweight='bold')\n",
    "ax.set_xlabel('Clusters', fontsize='large', fontweight='bold')\n",
    "ax.set_ylabel('ID counts', fontsize='large', fontweight='bold')\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure size\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Scatterplot\n",
    "plt.scatter(pca_minmax_df.iloc[:,0], pca_minmax_df.iloc[:,1], c=df['Hierarchical_labels'], cmap=\"brg\", s=40)\n",
    "\n",
    "# Aesthetics\n",
    "plt.title('PCA plot in 2D')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a766f7c",
   "metadata": {},
   "source": [
    "#### We can now clearly see the 3 number of clusters formed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75364df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = data.copy()\n",
    "df1.rename({\"Hierarchical_labels\":'Cluster'}, axis=1,inplace=True)\n",
    "\n",
    "aux=df1.columns.tolist()\n",
    "aux[0:len(aux)-1]\n",
    "\n",
    "sns.set(style=\"darkgrid\", rc={'figure.figsize':(13,8)}, font_scale=2)\n",
    "\n",
    "for cluster in aux[0:len(aux)-1]:\n",
    "    grid = sns.FacetGrid(df1, col=\"Cluster\")\n",
    "    grid.map(plt.hist, cluster, color=\"skyblue\", lw=1, edgecolor=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7d8c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting elements based on cluster label assigned and taking average for insights.\n",
    "\n",
    "cluster1 = pd.DataFrame(df1.loc[df1.Cluster==0].mean(),columns= ['Cluster1_avg'])\n",
    "cluster2 = pd.DataFrame(df1.loc[df1.Cluster==1].mean(),columns= ['Cluster2_avg'])\n",
    "cluster3 = pd.DataFrame(df1.loc[df1.Cluster==2].mean(),columns= ['Cluster2_avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e808e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df = pd.concat([cluster1,cluster2,cluster3],axis=1)\n",
    "avg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de752d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and plot one Column data .xs method\n",
    "for i , row in avg_df.iterrows():\n",
    "    fig = plt.subplots(figsize=(8,6))\n",
    "    j = avg_df.xs(i ,axis = 0)\n",
    "    plt.title(i, fontsize=16, fontweight=20)\n",
    "    j.plot(kind='bar',fontsize=14)\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e515f9",
   "metadata": {},
   "source": [
    "### 8.6(A) Comparing Hierarchical Clusters with the Original Classified Dataset using \"Type\" Feature<a class=\"anchor\" id=\"8.6A\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d3a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot\n",
    "plt.scatter(pca_minmax_df.iloc[:,0], pca_minmax_df.iloc[:,1], c=df['Hierarchical_labels'], cmap=\"brg\", s=40)\n",
    "\n",
    "# Aesthetics\n",
    "plt.title('After Hierarchical Clustering on PCA MinMax Scaled Dataset')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()\n",
    "\n",
    "# Scatterplot\n",
    "plt.scatter(pca_minmax_df.iloc[:,0], pca_minmax_df.iloc[:,1], c=data['Type'], cmap=\"brg\", s=40)\n",
    "\n",
    "# Aesthetics\n",
    "plt.title('Original Classification without Hierarchical CLustering')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39101c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by Clusters (K=3)\n",
    "display('After Applying Hierarchical CLustering on  PCA MinMax Scaled Dataset',df.groupby('Hierarchical_labels').agg(['mean']),'Original Classified Dataset',data.groupby('Type').agg(['mean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038d27c0",
   "metadata": {},
   "source": [
    "# 9. Conclusion:<a class=\"anchor\" id=\"9\"></a>\n",
    "\n",
    "##### I have applied EDA to analyze dataset.Discovered correlation between diff variables and found colinearity.\n",
    "##### Applied Standardazation & MinMaxScalar transformation on the data to use Principle componets analysis effectively.\n",
    "##### I have used & analyzed two clustering techniques here..i) KMeans  ii) Hierarchical Clusterig\n",
    "\n",
    "##### By applying clustering on diff. PCA obtained with diff transformation data shows fluctuation in model score. So finally the Standard Scaler found less score so not used for further model building."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
